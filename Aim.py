# --- Import Libraries ---
import streamlit as st
import pandas as pd
import io
from datetime import datetime
import numpy as np
import traceback
import re
import os # For environment variables
import google.generativeai as genai # Import Gemini library

# --- Page Configuration ---
st.set_page_config(
    page_title="AI SQL Generator",
    page_icon="🤖",
    layout="wide",
    initial_sidebar_state="collapsed"
)

# --- General Configuration ---
HEADER_ROW_ZERO_INDEXED = 0

# --- Gemini AI Configuration ---
# Use Streamlit secrets or environment variables for API keys in deployed apps
# For local development, ensure the GOOGLE_API_KEY environment variable is set.
try:
    # ---> IMPORTANT: Store your API Key as an environment variable <---
    # Example for local run: export GOOGLE_API_KEY='YOUR_API_KEY'
    # In Streamlit Cloud, set it in the app's secrets manager.
    GOOGLE_API_KEY = AIzaSyCC2LKSvLd7_u6RSsWEAmtHsvZNiUPphoI
    if not GOOGLE_API_KEY:
        # Attempt to get from Streamlit secrets if running in Cloud
        try:
            GOOGLE_API_KEY = st.secrets["GOOGLE_API_KEY"]
        except:
             pass # Keep GOOGLE_API_KEY as None if not found

    if not GOOGLE_API_KEY:
        st.error("🚨 Google API Key not found. Please set the GOOGLE_API_KEY environment variable or add it to Streamlit secrets.", icon="🔑")
        # Allow app to load but disable AI features later
        GEMINI_CONFIGURED = False
    else:
        genai.configure(api_key=GOOGLE_API_KEY)
        # Choose the Gemini model
        model = genai.GenerativeModel('gemini-1.5-flash')
        GEMINI_SAFETY_SETTINGS = [
            {"category": "HARM_CATEGORY_HARASSMENT", "threshold": "BLOCK_MEDIUM_AND_ABOVE"},
            {"category": "HARM_CATEGORY_HATE_SPEECH", "threshold": "BLOCK_MEDIUM_AND_ABOVE"},
            {"category": "HARM_CATEGORY_SEXUALLY_EXPLICIT", "threshold": "BLOCK_MEDIUM_AND_ABOVE"},
            {"category": "HARM_CATEGORY_DANGEROUS_CONTENT", "threshold": "BLOCK_MEDIUM_AND_ABOVE"},
        ]
        GEMINI_CONFIGURED = True
        print(f"✅ Gemini AI Configured with model: {model.model_name}") # For server logs

except Exception as e:
    st.error(f"🚨 Error configuring Google Gemini: {e}", icon="🔥")
    st.info("Ensure the 'google-generativeai' library is installed (`pip install google-generativeai`) and the API key is correctly set.")
    GEMINI_CONFIGURED = False


# --- Helper function to escape single quotes ---
def escape_sql_string(value):
    """Escapes single quotes in a string for SQL compatibility."""
    if isinstance(value, str):
        return value.replace("'", "''")
    return value

# --- Function to create and provide the template file (Property Mapping) ---
@st.cache_data # Cache the template generation
def get_template_excel_bytes():
    """Creates an Excel template file in memory for Property Mapping."""
    template_headers = [
        "Provider", "Source_Pty_Id", "AIM Code", "AIM Property Name",
        "Pty_iTarget_Pty_Idd", "Ext_Id"
    ]
    example_data = {
         "Provider": ["ExampleProvider"], "Source_Pty_Id": ["SRC100"],
         "AIM Code": ["AIM100"], "AIM Property Name": ["Example Property One"],
         "Pty_iTarget_Pty_Idd": ["12345"], "Ext_Id": ["EXT100"]
     }
    df_template = pd.DataFrame(example_data, columns=template_headers)
    output = io.BytesIO()
    with pd.ExcelWriter(output, engine='xlsxwriter') as writer:
        df_template.to_excel(writer, index=False, sheet_name='MappingData')
    output.seek(0)
    return output.getvalue()

# --- Function to Generate SQL using Gemini AI ---
def generate_sql_with_gemini(operation_type, user_inputs):
    """Generates SQL script using Gemini AI. Uses st elements for feedback."""
    if not GEMINI_CONFIGURED:
        st.error("Gemini AI is not configured. Cannot generate SQL.")
        return None, "AI not configured."

    prompt = ""
    script_header = f"""-- ======================================================================
-- AI GENERATED SQL SCRIPT - **REVIEW CAREFULLY BEFORE EXECUTION**
-- Generated by: Google Gemini ({model.model_name}) via Streamlit App
-- Generation Time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
-- Operation Type: {operation_type}
"""
    COL_SOURCE_ID_HDR = "Source_Pty_Id" # Keep consistent with processing func
    COL_TARGET_ID_HDR = "Pty_iTarget_Pty_Idd"
    COL_AIM_NAME_HDR = "AIM Property Name"

    status_placeholder = st.empty() # To show progress

    try:
        # --- Construct Prompt (Identical logic to Colab version) ---
        if operation_type == "Property Mapping":
            df = user_inputs.get('filtered_df')
            filename = user_inputs.get('filename', 'Unknown File')
            if df is None or df.empty: return None, "No data rows provided to AI."
            csv_data = df.to_csv(index=False)
            if len(csv_data) > 15000: st.warning(f"⚠️ Input data for AI is large ({len(csv_data)} chars).")
            script_header += f"-- Source File: {filename}\n-- Target Table: admin.PropertyMapping\n-- Filter Applied: Source_Pty_Id == AIM Code == Ext_Id (non-blank) AND Pty_iTarget_Pty_Idd is numeric\n-- ======================================================================\n\n"
            prompt = f"""
            You are an AI assistant generating T-SQL scripts for Microsoft SQL Server. Task: Create an SQL script to insert records into the 'admin.PropertyMapping' table based on the provided data. Table Schema: admin.PropertyMapping (Source_Pty_Id VARCHAR(255) PRIMARY KEY, Target_Pty_Id INT PRIMARY KEY, Active BIT, Created DATETIME) Input Data (CSV format): ```csv\n{csv_data}\n``` Instructions for SQL Generation: 1. For EACH row in the CSV data: 2. Generate a SQL block that FIRST checks if a record with the same 'Source_Pty_Id' AND 'Target_Pty_Id' already exists in 'admin.PropertyMapping'. Use the exact column names from the CSV header ('{COL_SOURCE_ID_HDR}', '{COL_TARGET_ID_HDR}'). 3. If the record DOES NOT exist, INSERT a new row. 4. The INSERT statement should map CSV columns to table columns as follows: * '{COL_SOURCE_ID_HDR}' from CSV -> `Source_Pty_Id` table column (VARCHAR). Escape single quotes if necessary. * '{COL_TARGET_ID_HDR}' from CSV -> `Target_Pty_Id` table column (INT). Ensure this is treated as a number. * Set the `Active` table column to `1`. * Set the `Created` table column to the current database time using `GETDATE()`. 5. Include comments in the SQL block indicating the Property Name ('{COL_AIM_NAME_HDR}') and Source ID for context. 6. Include a `SELECT` statement *before* the `IF NOT EXISTS` check and *after* the potential `INSERT` (inside the `END` if using `BEGIN/END`) to verify the state for that specific mapping. 7. Wrap the check and insert in an `IF NOT EXISTS (...) BEGIN ... END` structure. 8. Ensure correct T-SQL syntax. Example SQL block structure for one row: -- Property: [AIM Property Name] (Source ID: [{COL_SOURCE_ID_HDR}]) SELECT * FROM admin.PropertyMapping WHERE Source_Pty_Id = '...' AND Target_Pty_Id = ...; IF NOT EXISTS (SELECT 1 FROM admin.PropertyMapping WHERE Source_Pty_Id = '...' AND Target_Pty_Id = ...) BEGIN INSERT INTO admin.PropertyMapping (Source_Pty_Id, Target_Pty_Id, Active, Created) VALUES ('...', ..., 1, GETDATE()); SELECT * FROM admin.PropertyMapping WHERE Source_Pty_Id = '...' AND Target_Pty_Id = ...; END Generate the complete script containing blocks for all rows in the provided CSV data.
            """ # Prompt kept compact for brevity

        elif operation_type == "DMG Data Cleanup":
            client_db = user_inputs.get('client_db')
            start_period = user_inputs.get('start_period')
            end_period = user_inputs.get('end_period')
            if not client_db or not start_period or not end_period: return None, "Missing required inputs (DB Name, Start Period, End Period)."
            if not (start_period.isdigit() and len(start_period) == 8 and end_period.isdigit() and len(end_period) == 8) or int(start_period) > int(end_period): return None, "Invalid period format or range."
            safe_client_db = f"[{client_db.replace(']', ']]')}]"
            script_header += f"-- Target Database: {safe_client_db}\n-- Target Table: CashFlow (via Entity join)\n-- Filter: EntityType = 'Asset' AND Period BETWEEN {start_period} AND {end_period}\n-- Action: DELETE matching records\n-- **WARNING: THIS SCRIPT PERFORMS DELETIONS. REVIEW VERY CAREFULLY.**\n-- ======================================================================\n\n"
            prompt = f"""
            You are an AI assistant generating T-SQL scripts for Microsoft SQL Server. Task: Create an SQL script to delete specific records from the 'CashFlow' table within a specified client database. Target Database: {safe_client_db} Relevant Tables & Columns: * CashFlow (EntityKey INT, Period INT, ...) * Entity (EntityKey INT, EntityType VARCHAR(50), ...) Instructions for SQL Generation: 1. Switch context to the target database: `USE {safe_client_db}; GO`. 2. **Before** the deletion, include a `SELECT C.*` statement to show the records that *will be* deleted. Join `CashFlow` (C) with `Entity` (E) on `C.EntityKey = E.EntityKey`. Filter where `E.EntityType = 'Asset'` AND `C.Period` is BETWEEN {start_period} AND {end_period}. 3. Perform the `DELETE` operation on `CashFlow` (using alias C) based on the *exact same* join and filter criteria. 4. **After** the deletion, include the *exact same* `SELECT C.*` statement again to verify. 5. Use `GO` batch separators. Ensure correct T-SQL syntax. Generate the complete SQL script following these steps precisely. Include comments explaining each step (SELECT before, DELETE, SELECT after).
            """ # Prompt kept compact

        elif operation_type == "AIM Data Cleanup":
            aim_db = user_inputs.get('aim_db')
            period = user_inputs.get('period')
            if not aim_db or not period: return None, "Missing required inputs (AIM DB Name, Period)."
            if not re.fullmatch(r"^\d{4}[Mm][Tt][Hh]\d{2}$", period): return None, "Invalid AIM period format (YYYYMTHMM)."
            safe_aim_db = f"[{aim_db.replace(']', ']]')}]"; safe_period = escape_sql_string(period)
            script_header += f"-- Target Database: {safe_aim_db}\n-- Target Table: line_item\n-- Filter: item_typ_id IN (SELECT acct_id FROM account) AND period = '{safe_period}'\n-- Action: DELETE matching records\n-- **WARNING: THIS SCRIPT PERFORMS DELETions. REVIEW VERY CAREFULLY.**\n-- ======================================================================\n\n"
            prompt = f"""
            You are an AI assistant generating T-SQL scripts for Microsoft SQL Server. Task: Create an SQL script to delete specific records from the 'line_item' table within a specified AIM database. Target Database: {safe_aim_db} Relevant Tables & Columns: * line_item (item_typ_id INT, period VARCHAR(50), ...) * account (acct_id INT, ...) Instructions for SQL Generation: 1. Switch context to the target database: `USE {safe_aim_db}; GO`. 2. **Before** the deletion, include a `SELECT *` from `line_item` to show records where `item_typ_id` is IN `(SELECT acct_id FROM account)` AND the `period` column = '{safe_period}'. 3. Perform the `DELETE` operation on `line_item` based on the *exact same* filter criteria. 4. **After** the deletion, include the *exact same* `SELECT *` statement again to verify. 5. Use `GO` batch separators. Ensure correct T-SQL syntax. Generate the complete SQL script following these steps precisely. Include comments explaining each step (SELECT before, DELETE, SELECT after).
            """ # Prompt kept compact
        else:
            return None, f"Unsupported operation type: {operation_type}"

        if not prompt: return None, "Internal error: Prompt not constructed."

        status_placeholder.info(f"⏳ Sending request to Gemini AI ({model.model_name}) for {operation_type}...")

        response = model.generate_content(prompt, safety_settings=GEMINI_SAFETY_SETTINGS)

        # Handle potential blocks or errors in response
        if not response.candidates:
             try:
                 block_reason = response.prompt_feedback.block_reason
                 block_details = response.prompt_feedback.block_reason_message
                 st.error(f"🚨 Gemini AI request blocked. Reason: {block_reason}. Details: {block_details}", icon="🚫")
                 return None, f"AI request blocked: {block_reason}"
             except Exception:
                 st.error("🚨 Gemini AI returned an empty response or was blocked. Check API console.", icon="🚫")
                 return None, "AI returned an empty or blocked response."

        generated_sql = response.text
        # Basic cleanup
        generated_sql = re.sub(r"^```sql\s*", "", generated_sql, flags=re.IGNORECASE | re.MULTILINE)
        generated_sql = re.sub(r"\s*```$", "", generated_sql, flags=re.IGNORECASE | re.MULTILINE)
        final_script = script_header + generated_sql.strip()

        status_placeholder.success(f"✅ AI generated SQL script for {operation_type}.")
        return final_script, None

    except Exception as e:
        status_placeholder.error(f"🚨 Error during AI generation: {e}", icon="🔥")
        # st.exception(e) # Optionally show full traceback in UI
        print(f"Gemini Generation Error Traceback: {traceback.format_exc()}") # Log for server/debug
        return None, f"AI generation failed: {str(e)}"
    finally:
        # Optionally clear the status placeholder after a delay or keep success/error message
        # status_placeholder.empty() # Clears the message immediately
        pass

# --- Property Mapping Specific Configuration & Processing ---
COL_PROVIDER_HDR = "Provider"
COL_SOURCE_ID_HDR = "Source_Pty_Id"
COL_AIM_CODE_HDR = "AIM Code"
COL_AIM_NAME_HDR = "AIM Property Name"
COL_TARGET_ID_HDR = "Pty_iTarget_Pty_Idd"
COL_EXT_ID_HDR = "Ext_Id"
REQUIRED_HEADERS_PM = [
    COL_PROVIDER_HDR, COL_SOURCE_ID_HDR, COL_AIM_CODE_HDR,
    COL_AIM_NAME_HDR, COL_TARGET_ID_HDR, COL_EXT_ID_HDR
]

def process_property_mapping(uploaded_file_obj):
    """Handles the Property Mapping process using AI. Uses st for feedback."""
    processed_data = None; error_message = None; rows_read = 0; rows_filtered = 0
    filename = uploaded_file_obj.name
    status_placeholder = st.empty() # Placeholder for status messages

    try:
        status_placeholder.info(f"--- Processing Property Mapping for: *{filename}* ---")

        # --- Stage 1 & 2: Read & Validate Headers ---
        file_content_bytes = uploaded_file_obj.getvalue()
        file_content = io.BytesIO(file_content_bytes)
        status_placeholder.info("Reading headers...")
        try:
            df_header_check = pd.read_excel(file_content, header=HEADER_ROW_ZERO_INDEXED, nrows=0, engine='openpyxl')
        except ImportError:
            st.warning("openpyxl not found, using default engine.", icon="⚠️")
            file_content.seek(0); df_header_check = pd.read_excel(file_content, header=HEADER_ROW_ZERO_INDEXED, nrows=0)
        except Exception as header_err:
             st.error(f"🚨 Error reading header row: {header_err}")
             return None, f"Could not read header row: {header_err}", 0, 0

        actual_headers = df_header_check.columns.tolist()
        header_map = {hdr.lower().strip(): hdr for hdr in actual_headers}
        col_indices = {}; missing_cols = []; found_cols_display = []; all_found = True
        status_placeholder.info("Validating headers...")
        for req_hdr in REQUIRED_HEADERS_PM:
            req_hdr_lower = req_hdr.lower()
            if req_hdr_lower in header_map:
                original_case_hdr = header_map[req_hdr_lower]
                col_indices[req_hdr] = original_case_hdr; found_cols_display.append(f"  ✅ Found '{req_hdr}' (as '{original_case_hdr}')")
            else: missing_cols.append(f"  ❌ Missing '{req_hdr}'"); all_found = False

        with st.expander("Header Validation Details", expanded=not all_found):
            st.markdown("\n".join(found_cols_display + missing_cols))

        if not all_found:
            error_message = f"Header validation failed. Missing: {', '.join([h.split('**')[1].split('**')[0] for h in missing_cols if 'Missing' in h])}" # Simplified missing list
            st.error(f"🚨 {error_message}")
            status_placeholder.empty() # Clear status as error is shown
            return None, error_message, 0, 0

        st.success("✅ Header validation successful!")

        # --- Stage 3: Read Full Data ---
        status_placeholder.info("Reading full data...")
        file_content.seek(0)
        try:
            df = pd.read_excel(file_content, header=HEADER_ROW_ZERO_INDEXED, usecols=list(col_indices.values()), dtype=str, engine='openpyxl')
        except ImportError:
             file_content.seek(0); df = pd.read_excel(file_content, header=HEADER_ROW_ZERO_INDEXED, usecols=list(col_indices.values()), dtype=str)
        except Exception as read_err:
            st.error(f"🚨 Error reading full data: {read_err}")
            return None, f"Error reading data: {read_err}", 0, 0

        df = df.fillna(''); rows_read = len(df)
        status_placeholder.info(f"Read {rows_read} data rows. Applying filters...")

        # --- Stage 4: Data Processing and Filtering ---
        reverse_header_map = {v: k for k, v in col_indices.items()}; df_processed = df.rename(columns=reverse_header_map)
        df_processed[COL_SOURCE_ID_HDR] = df_processed[COL_SOURCE_ID_HDR].astype(str).str.strip()
        df_processed[COL_AIM_CODE_HDR] = df_processed[COL_AIM_CODE_HDR].astype(str).str.strip()
        df_processed[COL_EXT_ID_HDR] = df_processed[COL_EXT_ID_HDR].astype(str).str.strip()
        df_processed[COL_AIM_NAME_HDR] = df_processed[COL_AIM_NAME_HDR].astype(str).str.strip()
        df_processed[COL_TARGET_ID_HDR] = pd.to_numeric(df_processed[COL_TARGET_ID_HDR], errors='coerce')
        filter_mask = (df_processed[COL_SOURCE_ID_HDR] != '') & (df_processed[COL_SOURCE_ID_HDR] == df_processed[COL_AIM_CODE_HDR]) & (df_processed[COL_SOURCE_ID_HDR] == df_processed[COL_EXT_ID_HDR]) & df_processed[COL_TARGET_ID_HDR].notna()
        filtered_df = df_processed[filter_mask].copy()
        if not filtered_df.empty: filtered_df.loc[:, COL_TARGET_ID_HDR] = filtered_df[COL_TARGET_ID_HDR].astype(int)
        rows_filtered = len(filtered_df)
        status_placeholder.info(f"Found {rows_filtered} rows matching criteria. Preparing for AI...")

        # --- Stage 5: Generate SQL Script using AI ---
        if not filtered_df.empty:
            ai_inputs = {'filtered_df': filtered_df[[COL_SOURCE_ID_HDR, COL_TARGET_ID_HDR, COL_AIM_NAME_HDR]], 'filename': filename}
            # AI function will show its own status messages
            processed_data, error_msg = generate_sql_with_gemini("Property Mapping", ai_inputs)
            if error_msg: error_message = error_msg # Error already shown by AI func
            status_placeholder.empty() # Clear processing status, AI status remains
        else:
            status_placeholder.warning("⚠️ No data rows matched the filter criteria. No script generated.")
            error_message = "No matching rows found for Property Mapping."

    except Exception as e:
        status_placeholder.error(f"🚨 An error occurred during Property Mapping processing: {e}", icon="🔥")
        st.exception(e) # Show traceback in UI
        error_message = f"Unexpected error: {str(e)}"
        processed_data = None

    # Don't clear status_placeholder here if you want the final status (like 'no rows') to remain.
    # status_placeholder.empty()
    return processed_data, error_message, rows_read, rows_filtered


# --- DMG Data Cleanup Specific Functions ---
def process_dmg_cleanup(client_db, start_period, end_period):
    """Handles the process for DMG Data Cleanup using AI. Uses st for feedback."""
    processed_data = None; error_message = None
    status_placeholder = st.empty()

    try:
        status_placeholder.info(f"--- Processing DMG Data Cleanup: DB='{client_db}', Period='{start_period}-{end_period}' ---")
        # Input Validation (simplified checks, AI func does thorough checks)
        if not client_db or not start_period or not end_period:
            raise ValueError("DB Name, Start Period, End Period required.")

        status_placeholder.info("Inputs look okay. Preparing for AI...")
        ai_inputs = {'client_db': client_db, 'start_period': start_period, 'end_period': end_period}
        processed_data, error_msg = generate_sql_with_gemini("DMG Data Cleanup", ai_inputs)
        if error_msg: error_message = error_msg # Error shown by AI func
        status_placeholder.empty()

    except ValueError as ve:
        status_placeholder.error(f"🚨 Input validation failed: {ve}")
        error_message = f"Validation failed: {ve}"
    except Exception as e:
        status_placeholder.error(f"🚨 An error occurred during DMG Cleanup processing: {e}", icon="🔥")
        st.exception(e)
        error_message = f"Unexpected error: {str(e)}"

    # status_placeholder.empty()
    return processed_data, error_message

# --- AIM Data Cleanup Specific Functions ---
def process_aim_cleanup(aim_db, period):
    """Handles the process for AIM Data Cleanup using AI. Uses st for feedback."""
    processed_data = None; error_message = None
    status_placeholder = st.empty()

    try:
        status_placeholder.info(f"--- Processing AIM Data Cleanup: DB='{aim_db}', Period='{period}' ---")
         # Input Validation (simplified checks, AI func does thorough checks)
        if not aim_db or not period:
            raise ValueError("AIM DB Name and Period required.")
        if not re.fullmatch(r"^\d{4}[Mm][Tt][Hh]\d{2}$", period):
            raise ValueError("Period must be YYYYMTHMM format.")

        status_placeholder.info("Inputs look okay. Preparing for AI...")
        ai_inputs = {'aim_db': aim_db, 'period': period}
        processed_data, error_msg = generate_sql_with_gemini("AIM Data Cleanup", ai_inputs)
        if error_msg: error_message = error_msg # Error shown by AI func
        status_placeholder.empty()

    except ValueError as ve:
        status_placeholder.error(f"🚨 Input validation failed: {ve}")
        error_message = f"Validation failed: {ve}"
    except Exception as e:
        status_placeholder.error(f"🚨 An error occurred during AIM Cleanup processing: {e}", icon="🔥")
        st.exception(e)
        error_message = f"Unexpected error: {str(e)}"

    # status_placeholder.empty()
    return processed_data, error_message

# ==============================================================================
# --- Streamlit App UI ---
# ==============================================================================

st.title("🤖 AI-Powered SQL Script Generator")

st.markdown("Automate T-SQL script creation using **Google Gemini AI**. Select an operation, provide inputs, and let the AI generate the script.")
st.warning("""
    **⚠️ Important:** SQL scripts are generated by AI. **ALWAYS review the generated script VERY CAREFULLY for correctness and safety before executing it on any database.**
    You are responsible for validating the AI's output. Use caution, especially with `DELETE` operations.
    Ensure your `GOOGLE_API_KEY` environment variable (or Streamlit secret) is set. API usage may incur costs.
    """, icon="🚨")

if not GEMINI_CONFIGURED:
    st.error("🔴 Gemini AI is not configured. Please set the API key. SQL generation is disabled.", icon="🚫")
    # Optionally stop the rest of the UI from loading
    # st.stop()

st.divider()

# --- Initialize Session State ---
# We need to store results and input states to survive Streamlit reruns
if 'operation' not in st.session_state: st.session_state.operation = "Select..."
if 'generated_sql' not in st.session_state: st.session_state.generated_sql = None
if 'error_message' not in st.session_state: st.session_state.error_message = None
if 'rows_read' not in st.session_state: st.session_state.rows_read = 0
if 'rows_filtered' not in st.session_state: st.session_state.rows_filtered = 0
if 'processed_identifier' not in st.session_state: st.session_state.processed_identifier = None # Filename or params
# Input field states
if 'dmg_client_db' not in st.session_state: st.session_state.dmg_client_db = ""
if 'dmg_start_period' not in st.session_state: st.session_state.dmg_start_period = ""
if 'dmg_end_period' not in st.session_state: st.session_state.dmg_end_period = ""
if 'aim_db_name' not in st.session_state: st.session_state.aim_db_name = ""
if 'aim_period' not in st.session_state: st.session_state.aim_period = ""
if 'uploader_key' not in st.session_state: st.session_state.uploader_key = 0 # To reset file uploader


# --- Callback to Reset State on Operation Change ---
def reset_state():
    st.session_state.generated_sql = None
    st.session_state.error_message = None
    st.session_state.rows_read = 0
    st.session_state.rows_filtered = 0
    st.session_state.processed_identifier = None
    # Optionally reset input fields too, or keep them for convenience? Let's keep them for now.
    st.session_state.uploader_key += 1 # Force file uploader reset


# --- Step 1: Select Operation ---
st.subheader("Step 1: Select Operation Type")
operation_options = [
    "Select...",
    "Property Mapping",
    "DMG Data Cleanup",
    "AIM Data Cleanup",
]
selected_operation = st.selectbox(
    "Select the task for the AI:",
    options=operation_options,
    key='operation_selector', # Use a key to easily access the value
    on_change=reset_state # Reset results when selection changes
)
st.session_state.operation = selected_operation # Store selection in state

# --- Step 2: Instructions and Inputs ---
st.subheader("Step 2: Provide Inputs & Instructions")

with st.expander("ℹ Instructions and Inputs", expanded=True):

    if selected_operation == "Select...":
        st.info("Choose an operation from the dropdown above.")

    # --- Property Mapping UI ---
    elif selected_operation == "Property Mapping":
        st.markdown(f"""
            **Instructions for Property Mapping (AI Generated):**
            1.  **Prepare Excel File:** Use `.xlsx`/`.xls` format. Headers in Row {HEADER_ROW_ZERO_INDEXED + 1}.
            2.  **Required Headers:** `{', '.join([f'**{h}**' for h in REQUIRED_HEADERS_PM])}` (case-insensitive matching).
            3.  **Template:** Download the template below to ensure correct structure.
            4.  **Upload:** Use 'Browse files' below.
            5.  **Filtering:** The tool filters data *before* sending to AI: `Source_Pty_Id` == `AIM Code` == `Ext_Id` (non-blank) AND `Pty_iTarget_Pty_Idd` is numeric.
            6.  **AI Generation:** Click 'Generate Script'. Filtered data is sent to Gemini AI.
            7.  **⚠️ REVIEW SCRIPT:** Carefully examine the generated SQL before use.
        """)
        st.markdown("**Download Template:**")
        st.download_button(
            label="📄 Download Property Mapping Template (.xlsx)",
            data=get_template_excel_bytes(), # Use cached function
            file_name="PropertyMapping_Template.xlsx",
            mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
        )
        st.markdown("---")
        uploaded_file = st.file_uploader(
            "Upload your completed Excel file",
            type=['xlsx', 'xls'],
            key=f"uploader_{st.session_state.uploader_key}", # Use key to allow reset
            help="Ensure the file follows the template structure."
        )

    # --- DMG Data Cleanup UI ---
    elif selected_operation == "DMG Data Cleanup":
        st.markdown("""
            **Instructions for DMG Data Cleanup (AI Generated):**
            1.  **Inputs:** Provide Client DB Name and Period Range (YYYYMMDD).
            2.  **AI Generation:** Click 'Generate Script'. AI creates a script targeting the DB.
            3.  **Expected AI Logic:** SELECT matching `CashFlow` (`EntityType = 'Asset'`, period range), DELETE them, SELECT again to verify.
            4.  **🔥 CRITICAL REVIEW:** This script performs **DELETIONS**. Review VERY carefully (DB name, table, conditions, period).
        """)
        st.markdown("---")
        st.session_state.dmg_client_db = st.text_input(
            "Client Database Name:",
            key="dmg_db_input", # Use key to sync with session state
            value=st.session_state.dmg_client_db,
            placeholder="e.g., AegonDQSI",
            help="Enter the exact name of the target database."
        )
        col1, col2 = st.columns(2)
        with col1:
            st.session_state.dmg_start_period = st.text_input(
                "Start Period (YYYYMMDD):",
                key="dmg_start_input",
                value=st.session_state.dmg_start_period,
                placeholder="e.g., 20240101", max_chars=8,
                help="Inclusive start date."
            )
        with col2:
            st.session_state.dmg_end_period = st.text_input(
                "End Period (YYYYMMDD):",
                key="dmg_end_input",
                value=st.session_state.dmg_end_period,
                placeholder="e.g., 20240331", max_chars=8,
                help="Inclusive end date."
            )

    # --- AIM Data Cleanup UI ---
    elif selected_operation == "AIM Data Cleanup":
        st.markdown("""
            **Instructions for AIM Data Cleanup (AI Generated):**
            1.  **Inputs:** Provide AIM DB Name and Period (YYYYMTHMM).
            2.  **AI Generation:** Click 'Generate Script'. AI creates a script targeting the DB.
            3.  **Expected AI Logic:** SELECT `line_item` (`item_typ_id` in `account`, specific period), DELETE them, SELECT again to verify.
            4.  **🔥 CRITICAL REVIEW:** This script performs **DELETIONS**. Review VERY carefully (DB name, table, conditions, period).
        """)
        st.markdown("---")
        st.session_state.aim_db_name = st.text_input(
            "AIM Database Name:",
            key="aim_db_input",
            value=st.session_state.aim_db_name,
            placeholder="e.g., aim_1019",
            help="Enter the exact name of the AIM database."
        )
        st.session_state.aim_period = st.text_input(
            "Period (YYYYMTHMM):",
            key="aim_period_input",
            value=st.session_state.aim_period,
            placeholder="e.g., 2025MTH01", max_chars=9,
            help="Enter the specific period (case-insensitive 'MTH')."
        )

# --- Step 3: Generate Script ---
st.divider()
st.subheader("Step 3: Generate AI SQL Script")

# Determine if inputs are sufficient to enable the button
can_process = False
inputs_provided = False
if selected_operation == "Property Mapping" and 'uploaded_file' in locals() and uploaded_file is not None:
    inputs_provided = True
elif selected_operation == "DMG Data Cleanup" and st.session_state.dmg_client_db and st.session_state.dmg_start_period and st.session_state.dmg_end_period:
    inputs_provided = True
elif selected_operation == "AIM Data Cleanup" and st.session_state.aim_db_name and st.session_state.aim_period:
    inputs_provided = True

# Enable button only if AI is configured AND inputs are provided
can_process = GEMINI_CONFIGURED and inputs_provided

process_button = st.button(
    "🤖 Generate AI Script",
    disabled=not can_process,
    help="Requires API Key to be configured and all necessary inputs for the selected operation."
)

# --- Processing Logic ---
if process_button and can_process:
    # Clear previous results before starting new processing
    reset_state()

    with st.spinner(f"🧠 Processing '{selected_operation}' and asking Gemini AI..."):
        if selected_operation == "Property Mapping":
            sql, err, r_read, r_filt = process_property_mapping(uploaded_file)
            st.session_state.generated_sql = sql
            st.session_state.error_message = err
            st.session_state.rows_read = r_read
            st.session_state.rows_filtered = r_filt
            st.session_state.processed_identifier = uploaded_file.name

        elif selected_operation == "DMG Data Cleanup":
            sql, err = process_dmg_cleanup(
                st.session_state.dmg_client_db,
                st.session_state.dmg_start_period,
                st.session_state.dmg_end_period
            )
            st.session_state.generated_sql = sql
            st.session_state.error_message = err
            st.session_state.processed_identifier = f"DB: {st.session_state.dmg_client_db}, Period: {st.session_state.dmg_start_period}-{st.session_state.dmg_end_period}"

        elif selected_operation == "AIM Data Cleanup":
            sql, err = process_aim_cleanup(
                st.session_state.aim_db_name,
                st.session_state.aim_period
            )
            st.session_state.generated_sql = sql
            st.session_state.error_message = err
            st.session_state.processed_identifier = f"DB: {st.session_state.aim_db_name}, Period: {st.session_state.aim_period}"

        # Force a rerun after processing to update the results section based on session_state
        st.rerun()


# --- Step 4: Results ---
st.divider()
st.subheader("📊 AI Generation Results")

# Display results only if an operation has been selected and processed
if st.session_state.operation != "Select..." and st.session_state.processed_identifier is not None:
    op_done = st.session_state.operation
    identifier = st.session_state.processed_identifier

    st.markdown(f"**Results for: {op_done}** (Input: *{identifier}*)")

    if st.session_state.generated_sql:
        st.success(f"✅ Gemini AI successfully generated the SQL script!", icon="✨")
        if "DELETE" in st.session_state.generated_sql.upper():
             st.warning("🔥 **CAUTION: REVIEW DELETE SCRIPT BELOW VERY CAREFULLY BEFORE EXECUTION!** 🔥", icon="❗")
        else:
             st.warning("⚠️ **Review the generated SQL script below carefully before execution!**", icon="❗")


        # Display Metrics
        if op_done == "Property Mapping":
            m_col1, m_col2, m_col3 = st.columns(3)
            m_col1.metric("Rows Read (Input)", st.session_state.rows_read)
            m_col2.metric("Rows Filtered (Sent to AI)", st.session_state.rows_filtered)
            m_col3.metric("AI Script Generated", "Yes")
        else: # DMG, AIM don't have row counts from input like Prop Map
             st.metric("AI Script Generated", "Yes")


        st.code(st.session_state.generated_sql, language="sql")

        # Download Button
        file_name = f"AI_{op_done.replace(' ', '')}_Script_{datetime.now().strftime('%Y%m%d_%H%M%S')}.sql"
        st.download_button(
            label=f"📥 Download Generated SQL Script",
            data=st.session_state.generated_sql,
            file_name=file_name,
            mime="text/plain",
            help="Download the AI-generated SQL script. REVIEW BEFORE USE!"
        )

    elif st.session_state.error_message:
        # Display specific error messages
        if "No matching rows" in st.session_state.error_message and op_done == "Property Mapping":
            st.warning(f"⚠️ {st.session_state.error_message}. No data was sent to the AI.")
            m_col1, m_col2, m_col3 = st.columns(3)
            m_col1.metric("Rows Read (Input)", st.session_state.rows_read)
            m_col2.metric("Rows Filtered", 0)
            m_col3.metric("AI Script Generated", "No")
        else:
             st.error(f"❌ AI script generation failed. Error: {st.session_state.error_message}", icon="💔")
             if "AI request blocked" in st.session_state.error_message:
                 st.info("The request might have been blocked due to safety settings or potentially harmful content requests. Check the prompt structure or Google AI console.")
             # Optionally show input metrics if available and relevant
             if op_done == "Property Mapping" and st.session_state.rows_read > 0:
                 m_col1, m_col2, m_col3 = st.columns(3)
                 m_col1.metric("Rows Read", st.session_state.rows_read)
                 m_col2.metric("Rows Filtered", st.session_state.rows_filtered if st.session_state.rows_filtered else 'N/A')
                 m_col3.metric("AI Script Generated", "No")
             else:
                  st.metric("AI Script Generated", "No")


    else:
       # Should not happen if button was pressed and state was updated, but as fallback:
       st.info("Processing completed, but no script or error message was generated.")

elif st.session_state.operation != "Select...":
    st.info(f"Provide inputs for '{st.session_state.operation}' and click 'Generate AI Script'.")
else:
    st.info("Select an operation, provide inputs, and click 'Generate AI Script' to see results.")


# --- Footer ---
st.divider()
st.caption(f"AI SQL Generator | Powered by Google Gemini ({model.model_name if GEMINI_CONFIGURED else 'N/A'}) | v1.5")
