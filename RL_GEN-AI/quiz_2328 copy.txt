Question ID: 27235
Question: Which technique helps update the model's weights during training?
Options:
a: Cross-entropy loss
b: Gradient descent optimization
c: Tokenization strategy
d: Data normalization
Correct Option: b

Question ID: 27232
Question: What is the primary learning method used during pretraining of LLMs?
Options:
a: Supervised learning
b: Unsupervised learning
c: Reinforcement learning
d: Transfer learning
Correct Option: b

Question ID: 27229
Question: What is the first step in training Large Language Models (LLMs)?
Options:
a: Data Collection
b: Tokenization
c: Fine-Tuning
d: Optimization
Correct Option: a

Question ID: 27231
Question: What does the Masked Language Modeling objective focus on?
Options:
a: Predicting the next word
b: Predicting a missing (masked) word
c: Classifying text sentiment
d: Generating summaries
Correct Option: b

Question ID: 27237
Question: Which of the following is NOT a goal of pretraining?
Options:
a: Learning general knowledge
b: Developing a wide-ranging understanding of language
c: Focusing on specific downstream tasks
d: Acquiring knowledge about grammar and syntax
Correct Option: c

